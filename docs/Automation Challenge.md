# Automation Challenge

I would automate ingestion by orchestrating an `Airflow DAG` that uses an `Airflow sensor` such as a `FileSensor` or `PythonSensor` to continuously monitor the local data/ folder for newly arriving files. The sensor detects when a file appears and triggers downstream tasks only for files that have not been previously ingested, which is determined by comparing each file’s name and checksum against a metadata store. For each new file, the existing Python ETL script is executed to extract, clean, and transform the data before loading it into ClickHouse, where tables are designed with a deduplication-friendly engine like ReplacingMergeTree to ensure row-level idempotency in case overlapping records exist across files. After successful ingestion, the file’s metadata is updated to mark it as processed, preventing reprocessing. Using an Airflow sensor is preferable over a traditional file watcher because it integrates directly into the DAG orchestration, provides built-in retry, logging, and alerting mechanisms, scales well with multiple files and tasks, and avoids the complexity of running a separate persistent process outside Airflow.